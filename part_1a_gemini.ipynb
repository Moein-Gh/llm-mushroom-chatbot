{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path 1 - Gemini API\n",
    "\n",
    "This path will guide you on the set-up and usage of Google Gemini's API.\n",
    "\n",
    "### 0. Get and store the API key\n",
    "\n",
    "First of all, you need to login with your google account and get an API key [here](https://aistudio.google.com/app/apikey). It is **very important** that you do not share your API key with anyone and that you do not have it in your Repository.\n",
    "\n",
    "You can keep your API key in a secure local document and access it when needed. It is common to save the key as an environmental variable so that it can be accessed by your python script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this means your API key is in plain text in your script. To avoid this, if you're using VS Code, you can add your API key to a `.env` file in your workspace root with the following line:\n",
    "\n",
    "```sh\n",
    "API_KEY=\"PASTE YOUR KEY HERE\"\n",
    "```\n",
    "\n",
    "Alternatively, you can use the [dot-env library](https://github.com/theskumar/python-dotenv).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-z was unexpected at this time.\n"
     ]
    }
   ],
   "source": [
    "# You can check if the environment variable API_KEY has been set up properly by running this line\n",
    "!if [ -z $API_KEY ]; then echo \"\\$API_KEY not found\"; else echo \"\\$API_KEY found\"; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First simple request\n",
    "\n",
    "Now, you can write a simple script to see if everything is working properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import gradio as gr\n",
    "\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"])\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "model\n",
    "\n",
    "DEFAULT_CONFIG = genai.types.GenerationConfig(\n",
    "    # Only one candidate for now.\n",
    "    candidate_count=1,\n",
    "    stop_sequences=[\"\"],\n",
    "    max_output_tokens=50,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "\n",
    "def generateContent(prompt: str, config=DEFAULT_CONFIG):\n",
    "\n",
    "    response = model.generate_content(\n",
    "        prompt, generation_config=config or DEFAULT_CONFIG\n",
    "    )\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Ask the model to generate content about a random topic and print the response in text.\n",
    "\n",
    "Here is the [official documentation](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#configure) to find the help you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A pearl of silver, hung in night's embrace,\n",
      "The moon ascends, a silent, ethereal grace.\n",
      "Across the velvet canvas, she paints her glow,\n",
      "On mountains tall and seas that ebb and flow.\n",
      "\n",
      "She whispers secrets\n"
     ]
    }
   ],
   "source": [
    "result = generateContent(\"write a poem about the moon\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generation parameters\n",
    "\n",
    "When asking the model to generate some text, there are different parameters that you can tune to improve on the final quality of the text. [Here](https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters) is an overview of the parameters that Gemini offers. Try some of them in different context and understand how they affect the final generated text.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Play with the output temperature, which controls the randomness of the generated text `temperature=0` means deterministic output, while `temperature=1` means maximum randomness (try some intermediate value too) and keep the `max_output_tokens` to 50 so that the output is not too long.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Try out different `top_k` values, which controls how many tokens the model considers for output `top_k=1` means the model considers only one token for output (the one with the highest probability) `top_k=50` means the model considers the top 50 tokens for output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "The same exercise as before but now with `top_p`, which controls how the model selects tokens for output `top_p=0.1` means the model selects tokens that make up 10% of the cumulative probability mass `top_p=0.9` means the model selects tokens that make up 90% of the cumulative probability mass `top_p` filters tokens _after_ applying `top_k`.\n",
    "\n",
    "Can you determine a rule of thumb as to how `top_k` and `top_p` affect the output results? (If you can't try to push the values to extreme values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add images to the prompt\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "Gemini, beside text also accepts images (and videos). Try prompting it with one. Choose an interesting image and prompt the model with a query about it.\n",
    "\n",
    "You can use the [official documentation](https://ai.google.dev/gemini-api/docs/vision?lang=python#prompting-images).\n",
    "\n",
    "Use [PIL](https://pillow.readthedocs.io/en/stable/) to load an image. It should already be present in the Python environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myfile=genai.File({\n",
      "    'name': 'files/vj4dz8f4v9yf',\n",
      "    'display_name': 'London',\n",
      "    'mime_type': 'image/png',\n",
      "    'sha256_hash': 'NWI4NDRmOTgyYjdjNjgzMWE2OWQyNzcxNzc3MmNmYjA1M2I1OWQ0YTBmOTI2ZTk3YTM0ODIyNzI5OWQxZGNhMw==',\n",
      "    'size_bytes': '1405853',\n",
      "    'state': 'ACTIVE',\n",
      "    'uri': 'https://generativelanguage.googleapis.com/v1beta/files/vj4dz8f4v9yf',\n",
      "    'create_time': '2024-09-21T15:11:49.840691Z',\n",
      "    'expiration_time': '2024-09-23T15:11:49.822724426Z',\n",
      "    'update_time': '2024-09-21T15:11:49.840691Z'})\n",
      "The image shows a beautiful view of the Houses of Parliament and Big Ben in London, England. The iconic clock tower stands tall, its clock face clearly visible against the bright blue sky. The bridge in the foreground adds a sense of depth to the scene. The buildings are bathed in warm sunlight, creating a picturesque and serene atmosphere. The image captures the architectural beauty and historical significance of this landmark.\n"
     ]
    }
   ],
   "source": [
    "path = \"./image.png\"\n",
    "myfile = genai.upload_file(path=path, display_name=\"London\")\n",
    "print(f\"{myfile=}\")\n",
    "\n",
    "response = model.generate_content([myfile, \"Describe what you see in this image\"])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Document grounding\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "Depending on the application of the project, you might need to extract text from given documents. Gemini has this capability built-in. Choose an interesting document (or use the pdf in the data folder) to feed Gemini and prompt the model with a query about it. Extract the text in the pdf using the extract_text function of pdfminer, then ask Gemini (nicely) to summarize the document. Gemini will probably output some markdown output, which you can display using `display(Markdown(\"# Text\"))`.\n",
    "\n",
    "You can use the [official documentation](https://ai.google.dev/gemini-api/docs/document-processing?lang=python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper explores the use of \"chain-of-thought prompting\" to improve the reasoning abilities of large language models (LLMs). The authors propose augmenting standard prompting, where an LLM is given input-output pairs, with a chain of\n"
     ]
    }
   ],
   "source": [
    "DOC_PATH = \"./data/chain_of_thought_prompting.pdf\"\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Extract the text\n",
    "pdf_text = extract_text(DOC_PATH)\n",
    "\n",
    "result = generateContent(f'summerize this document: \"{pdf_text}\"')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explore on your own\n",
    "\n",
    "Gemini offers a bigger range of capabilities than those provided here. Explore them on your own!\n",
    "\n",
    "#### Exercise 7\n",
    "\n",
    "Explore!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5. Explore on your own: long chats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query or type 'e' to exit. Hello!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Hello!\n",
      "Assistant:  Ahoy, matey! What be yer pleasure? \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query or type 'e' to exit. e\n"
     ]
    }
   ],
   "source": [
    "# Re-Initialise the model, this time we are giving it a system instruction\n",
    "model = genai.GenerativeModel(\n",
    "    \"gemini-1.5-flash\",\n",
    "    system_instruction=\"You are a helpful pirate. Only reply with pirate jargon.\",\n",
    ")\n",
    "\n",
    "# We start with an empty chat history, but you can use this to e.g. provide examples for few-shot learning\n",
    "chat = model.start_chat(history=[])\n",
    "end_chat = False\n",
    "while not end_chat:\n",
    "    user_input = input(\"Enter your query or type 'e' to exit.\")\n",
    "    if user_input.lower().strip() == \"e\":\n",
    "        end_chat = True\n",
    "        break\n",
    "    response = chat.send_message(user_input)\n",
    "    print(\"User: \", user_input)\n",
    "    print(\"Assistant: \", response.text, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create a user interface\n",
    "\n",
    "#### Exercise 8\n",
    "\n",
    "Since you are trying to build a complete application, you also need a nice user interface that interacts with the model. There are various libraries available for this purpose. Notably: [gradio](https://www.gradio.app/docs/gradio/interface) and [chat UI](https://huggingface.co/docs/chat-ui/index). For the solution of this lab, we will use gradio.\n",
    "\n",
    "Gradio has pre-defined input/output blocks that are automatically inserted in the interface. You only need to provide an appropriate function that takes all the inputs and returns the relevant output. See documentation [here](https://www.gradio.app/docs/gradio/interface).\n",
    "\n",
    "Use a ChatInterface to create a chatbot UI that let's you discuss with Gemini, then add multimodal capabilities for both Gradio and Gemini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input={'text': 'hello', 'files': []}\n",
      "input={'text': 'what is in this picture', 'files': [{'path': '/tmp/gradio/116a1921477e2ec49f26721fef8bf9ab3908bad9c1749da933ec76dcc9f1c1fe/Screenshot 2024-07-17 081925.png', 'url': 'http://localhost:7860/file=/tmp/gradio/116a1921477e2ec49f26721fef8bf9ab3908bad9c1749da933ec76dcc9f1c1fe/Screenshot 2024-07-17 081925.png', 'size': 18871, 'orig_name': 'Screenshot 2024-07-17 081925.png', 'mime_type': 'image/png', 'is_stream': False, 'meta': {'_type': 'gradio.FileData'}}]}\n",
      "text='what is in this picture', file_path='/tmp/gradio/116a1921477e2ec49f26721fef8bf9ab3908bad9c1749da933ec76dcc9f1c1fe/Screenshot 2024-07-17 081925.png'\n",
      "input={'text': 'more', 'files': []}\n"
     ]
    }
   ],
   "source": [
    "# Answer here\n",
    "\n",
    "# This part closes the demo server if it is already running (which\n",
    "# happens easily in notebooks) and prevents you from opening multiple\n",
    "# servers at the same time.\n",
    "\n",
    "if \"demo\" in locals() and demo.is_running:\n",
    "    demo.close()\n",
    "\n",
    "\n",
    "# Edit the parameters below\n",
    "def echo(input, history):\n",
    "    print(f\"{input=}\")\n",
    "    text = input[\"text\"]\n",
    "\n",
    "    # Check if files are in input and handle file upload\n",
    "    if (\n",
    "        \"files\" in input\n",
    "        and isinstance(input[\"files\"], list)\n",
    "        and len(input[\"files\"]) > 0\n",
    "    ):\n",
    "        file_path = input[\"files\"][0][\"path\"]\n",
    "        print(f\"{text=}, {file_path=}\")\n",
    "\n",
    "        try:\n",
    "            myfile = genai.upload_file(path=file_path)\n",
    "            result = model.generate_content([myfile, text])\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading file: {e}\")\n",
    "            return \"Error processing the file.\"\n",
    "    else:\n",
    "        # If no file, process only the text input\n",
    "        result = model.generate_content([text])\n",
    "\n",
    "    return result.text\n",
    "\n",
    "\n",
    "# Create and launch the demo\n",
    "demo = gr.ChatInterface(\n",
    "    fn=echo,\n",
    "    multimodal=True,\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
