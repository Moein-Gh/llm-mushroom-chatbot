{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found.\n"
     ]
    }
   ],
   "source": [
    "genai.configure(api_key=os.environ[\"API_KEY\"])\n",
    "if not os.environ[\"API_KEY\"]:\n",
    "    print(\"API key not found. Please set the API_KEY environment variable.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"API key found.\")\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    \"gemini-1.5-flash-latest\",\n",
    "    system_instruction=f\"\"\"\n",
    "    Respond in style of a mushroom expert chatbot.\n",
    "    The messages should be only about mushrooms. \n",
    "    if the user asks about something else,you can ask that they ask about mushrooms instead.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/gradio/blocks.py:982: UserWarning: Cannot load dark. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/dark (Request ID: Root=1-66f18fac-5844a03460ac36af4b91ca28;5440ce20-a683-4733-a6c4-ecc328f120f3)\n",
      "\n",
      "Sorry, we can't find the page you are looking for.\n",
      "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if 'demo' is running and close it\n",
    "if \"demo\" in locals() and demo.is_running:\n",
    "    demo.close()\n",
    "\n",
    "# Start a new chat session with the model\n",
    "chat = model.start_chat(history=[])\n",
    "\n",
    "# Define the JSON request message\n",
    "json_request_message = \"\"\"\n",
    "Produce a JSON response with the following fields:\n",
    "- common_name\n",
    "- genus\n",
    "- confidence (of the prediction)\n",
    "- visible (what parts of the mushrooms are visible in the image, from the set {cap, hymenium, stipe})\n",
    "- color (of the mushroom in the picture)\n",
    "- edible (boolean indicating the edibility of the mushroom)\n",
    "\n",
    "Example JSON response:\n",
    "{\n",
    "  \"common_name\": \"Inkcap\",\n",
    "  \"genus\": \"Coprinus\",\n",
    "  \"confidence\": 0.5,\n",
    "  \"visible\": [\"cap\", \"hymenium\", \"stipe\"],\n",
    "  \"color\": \"orange\",\n",
    "  \"edible\": true\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Define the summary prompt\n",
    "summary_prompt = \"\"\"\n",
    "Provide a detailed summary of the image based on the previously provided JSON data.\n",
    "Mention all of the JSON properties except confidence.\n",
    "Do not mention the JSON file itself in the response.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def response(input_data, history):\n",
    "    global chat\n",
    "    text_input = input_data.get(\"text\", \"\")\n",
    "    files = input_data.get(\"files\", [])\n",
    "\n",
    "    # Check if a file is provided\n",
    "    if files and isinstance(files, list) and len(files) > 0:\n",
    "        file_path = files[0][\"path\"]\n",
    "        text_input = text_input or summary_prompt\n",
    "\n",
    "        try:\n",
    "            uploaded_file = genai.upload_file(path=file_path)\n",
    "\n",
    "            if text_input == summary_prompt:\n",
    "                # Generate JSON response based on the image\n",
    "                json_response = model.generate_content(\n",
    "                    [uploaded_file, json_request_message]\n",
    "                )\n",
    "                print(\"\\n-- JSON BASED ON IMAGE --\")\n",
    "                print(json_response.text)\n",
    "                print(\"-------------------------\")\n",
    "\n",
    "                # Append the JSON response to the chat history\n",
    "                chat.history.append({\"parts\": [json_response.text], \"role\": \"model\"})\n",
    "\n",
    "            # Send a message to the chat with the uploaded file and text input\n",
    "            gemini_response = chat.send_message(\n",
    "                {\"parts\": [uploaded_file, text_input], \"role\": \"user\"}, stream=True\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading file: {e}\")\n",
    "            error_message = (\n",
    "                \"An error occurred while uploading the file. \"\n",
    "                \"Please try again with a different input.\"\n",
    "            )\n",
    "            for i in range(len(error_message)):\n",
    "                time.sleep(0.01)\n",
    "                yield error_message[: i + 1]\n",
    "            return\n",
    "    else:\n",
    "        # If no file is provided, process only the text input\n",
    "        gemini_response = chat.send_message(\n",
    "            {\"parts\": [text_input], \"role\": \"user\"}, stream=True\n",
    "        )\n",
    "\n",
    "    # Resolve the response and print the chat history\n",
    "    gemini_response.resolve()\n",
    "    print(chat.history)\n",
    "\n",
    "    try:\n",
    "        # Check for safety ratings in the response\n",
    "        dangerous = any(\n",
    "            rating.probability != 1\n",
    "            for rating in gemini_response.candidates[0].safety_ratings\n",
    "        )\n",
    "\n",
    "        if dangerous:\n",
    "            # Rewind the chat history if the content is dangerous\n",
    "            chat.rewind()\n",
    "            warning_message = (\n",
    "                \"The model has detected that the content of the response may be harmful or inappropriate. \"\n",
    "                \"Please try again with a different input.\"\n",
    "            )\n",
    "            for i in range(len(warning_message)):\n",
    "                time.sleep(0.007)\n",
    "                yield warning_message[: i + 1]\n",
    "        else:\n",
    "            # Stream the response text to the user\n",
    "            for i in range(len(gemini_response.text)):\n",
    "                time.sleep(0.01)\n",
    "                yield gemini_response.text[: i + 1]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        error_message = (\n",
    "            \"An error has occurred. Please try again with a different input.\"\n",
    "        )\n",
    "        for i in range(len(error_message)):\n",
    "            time.sleep(0.007)\n",
    "            yield error_message[: i + 1]\n",
    "        chat.rewind()\n",
    "        return\n",
    "\n",
    "\n",
    "# Create and launch the Gradio interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=response,\n",
    "    multimodal=True,\n",
    "    theme=\"dark\",\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
