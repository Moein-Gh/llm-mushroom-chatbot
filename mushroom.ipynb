{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import time\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found.\n"
     ]
    }
   ],
   "source": [
    "genai.configure(api_key=os.environ[\"API_KEY\"])\n",
    "if not os.environ[\"API_KEY\"]:\n",
    "    print(\"API key not found. Please set the API_KEY environment variable.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"API key found.\")\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    \"gemini-1.5-flash-latest\",\n",
    "    system_instruction=f\"\"\"\n",
    "    Respond in style of a mushroom expert chatbot.\n",
    "    The messages should be only about mushrooms. \n",
    "    if the user asks about something else,you can ask that they ask about mushrooms instead.\n",
    "    keep the chat focused and compact and avoid chit-chat.\n",
    "    when asked to identify a mushroom, only provide the answer if you are confident, otherwise mention what you think and that you are not sure.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7870\n",
      "Running on local URL:  http://0.0.0.0:7870\n",
      "Running on public URL: https://e4882dff50b063585b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e4882dff50b063585b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- JSON BASED ON IMAGE --\n",
      "{\n",
      "  \"common_name\": \"Chanterelle\",\n",
      "  \"genus\": \"Cantharellus\",\n",
      "  \"confidence\": 0.9,\n",
      "  \"visible\": [\"cap\", \"hymenium\", \"stipe\"],\n",
      "  \"color\": \"orange\",\n",
      "  \"edible\": true\n",
      "} \n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check if 'demo' is running and close it\n",
    "if \"demo\" in locals() and demo.is_running:\n",
    "    demo.close()\n",
    "\n",
    "# Start a new chat session with the model\n",
    "chat = model.start_chat(history=[])\n",
    "\n",
    "# Define the JSON request message\n",
    "json_request_message = \"\"\"\n",
    "Produce a JSON response with the following fields:\n",
    "- common_name\n",
    "- genus\n",
    "- confidence (of the prediction)\n",
    "- visible (what parts of the mushrooms are visible in the image, from the set {cap, hymenium, stipe})\n",
    "- color (of the mushroom in the picture)\n",
    "- edible (boolean indicating the edibility of the mushroom)\n",
    "\n",
    "Example JSON response:\n",
    "{\n",
    "  \"common_name\": \"Inkcap\",\n",
    "  \"genus\": \"Coprinus\",\n",
    "  \"confidence\": 0.5,\n",
    "  \"visible\": [\"cap\", \"hymenium\", \"stipe\"],\n",
    "  \"color\": \"orange\",\n",
    "  \"edible\": true\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Define the summary prompt\n",
    "summary_prompt = \"\"\"\n",
    "Provide a detailed summary of the image based on the previously provided JSON data.\n",
    "Mention all of the JSON properties except confidence.\n",
    "Do not mention the JSON file itself in the response.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def response(input_data, history):\n",
    "    global chat\n",
    "    text_input = input_data.get(\"text\", \"\")\n",
    "    files = input_data.get(\"files\", [])\n",
    "\n",
    "    # Check if a file is provided\n",
    "    if files and isinstance(files, list) and len(files) > 0:\n",
    "        file_path = files[0][\"path\"]\n",
    "        text_input = text_input or summary_prompt\n",
    "\n",
    "        try:\n",
    "            uploaded_file = genai.upload_file(path=file_path)\n",
    "\n",
    "            if text_input == summary_prompt:\n",
    "                # Generate JSON response based on the image\n",
    "                json_response = model.generate_content(\n",
    "                    [uploaded_file, json_request_message]\n",
    "                )\n",
    "                print(\"\\n-- JSON BASED ON IMAGE --\")\n",
    "                print(json_response.text)\n",
    "                print(\"-------------------------\")\n",
    "\n",
    "                # Append the JSON response to the chat history\n",
    "                chat.history.append({\"parts\": [json_response.text], \"role\": \"model\"})\n",
    "\n",
    "            # Send a message to the chat with the uploaded file and text input\n",
    "            gemini_response = chat.send_message(\n",
    "                {\"parts\": [uploaded_file, text_input], \"role\": \"user\"}, stream=True\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading file: {e}\")\n",
    "            print(e)\n",
    "            error_message = (\n",
    "                \"An error occurred while uploading the file. \"\n",
    "                \"Please try again with a different input.\"\n",
    "            )\n",
    "            for i in range(len(error_message)):\n",
    "                time.sleep(0.01)\n",
    "                yield error_message[: i + 1]\n",
    "            return\n",
    "    else:\n",
    "        # If no file is provided, process only the text input\n",
    "        gemini_response = chat.send_message(\n",
    "            {\"parts\": [text_input], \"role\": \"user\"}, stream=True\n",
    "        )\n",
    "\n",
    "    # Resolve the response and print the chat history\n",
    "    gemini_response.resolve()\n",
    "\n",
    "    try:\n",
    "        # Check for safety ratings in the response\n",
    "        dangerous = any(\n",
    "            rating.probability != 1\n",
    "            for rating in gemini_response.candidates[0].safety_ratings\n",
    "        )\n",
    "\n",
    "        if dangerous:\n",
    "            # Rewind the chat history if the content is dangerous\n",
    "            chat.rewind()\n",
    "            warning_message = (\n",
    "                \"The model has detected that the content of the response may be harmful or inappropriate. \"\n",
    "                \"Please try again with a different input.\"\n",
    "            )\n",
    "            for i in range(len(warning_message)):\n",
    "                time.sleep(0.007)\n",
    "                yield warning_message[: i + 1]\n",
    "        else:\n",
    "            # Stream the response text to the user\n",
    "            for i in range(len(gemini_response.text)):\n",
    "                time.sleep(0.01)\n",
    "                yield gemini_response.text[: i + 1]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        error_message = (\n",
    "            \"An error has occurred. Please try again with a different input.\"\n",
    "        )\n",
    "        for i in range(len(error_message)):\n",
    "            time.sleep(0.007)\n",
    "            yield error_message[: i + 1]\n",
    "        chat.rewind()\n",
    "        return\n",
    "\n",
    "\n",
    "# Create and launch the Gradio interface\n",
    "demo = gr.ChatInterface(fn=response, multimodal=True, theme=\"Default\")\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
